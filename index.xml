<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://xiangxiaoxx.github.io/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 15 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://xiangxiaoxx.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>深度学习笔记</title>
      <link>https://xiangxiaoxx.github.io/posts/note/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://xiangxiaoxx.github.io/posts/note/</guid>
      <description>正则化 $g(w)=||w||1=\sum\limits{k}|w_i|$  $g(w)=||w||1^2=\sum\limits{k}|w_i^2|$
L1范数和L2范数 &amp;emsp;&amp;emsp;L1正则化会让参数变得更加稀疏，而L2正则化不会。 &amp;emsp;&amp;emsp;所谓参数变得稀疏是指会有更多参数变为0，这样可达到特征选取的功能。之所以L2正则化不会让参数变得稀疏的原因是，当参数很小时，这个参数的平方基本上就可以忽略了，于是模型不会进一步将这个参数调整为0。
偏置不需要正则化 &amp;emsp;&amp;emsp;同时，偏置是不应该被正则化的。正则化主要是为了防止过拟合，而过拟合一般表现为模型对于输出的微小改变产生巨大差异，而这主要是某些参数的w过大的原因，偏置对模型的曲率并没有任何贡献（模型的曲率是由w决定的，b对输入进行求导时，是直接约掉的），通过对||w||进行惩罚，才能缓解这种问题。 &amp;emsp;&amp;emsp;对||b||进行惩罚，其实是没有作用的。b对于所有对数据都是一视同仁的，而w会对不同的数据产生不一样的加权。 &amp;emsp;&amp;emsp;网上查了查相关的解释，还发现这是一道面试题。具体问法为，在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚。 &amp;emsp;&amp;emsp;《deep learning》书上的解释为：
&amp;gt;The biases typically require less data than the weights to ﬁt accurately. Each weight speciﬁes how two variables interact. Fitting the weight well requires observing both variables in a variety of conditions. Each bias controls only a single variable. This means that we do not induce too much variance by leaving the biases unregularized. Also, regularizing the bias parameter scan introduce a signiﬁcant amount of underﬁtting.</description>
    </item>
    
    <item>
      <title>一点日常</title>
      <link>https://xiangxiaoxx.github.io/about/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://xiangxiaoxx.github.io/about/</guid>
      <description>计算机专业在读硕士， 就读于上海交通大学，电子信息与电气工程学院。 目前的学习方向主要是&amp;#8195;&amp;#8195;情感计算(Affective Computing)，&amp;#8195;&amp;#8195;计算机视觉(Computer Vision)&amp;#8195;&amp;#8195;和深度学习(Deep Learning)。 本科就读于上海大学，上海电影学院数字媒体技术专业。 一直以来都比较喜欢记录生活，于是建立了这个博客。写下自己学习生涯、日常生活的感悟，给未来的自己看。</description>
    </item>
    
    <item>
      <title>DERA</title>
      <link>https://xiangxiaoxx.github.io/portfolio/dera/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://xiangxiaoxx.github.io/portfolio/dera/</guid>
      <description>This is a Java reference implementation of dynamic event-driven actors runtime aiming at providing adequate abstraction levels and mechanisms for modelling and developing (distributed) event-based systems. DERA leverages the intrinsic loose coupling of event-driven communication styles to support various kinds of run-time evolution and adaptation (i.e., enabling run-time flexibility) while minimizing the non-deterministic nature of traditional event-based applications (i.e., supporting formal analysis).</description>
    </item>
    
    <item>
      <title>hyde-hyde</title>
      <link>https://xiangxiaoxx.github.io/portfolio/hyde-hyde/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://xiangxiaoxx.github.io/portfolio/hyde-hyde/</guid>
      <description>Hyde-hyde is a responsive Hugo theme inspired by @spf13&amp;rsquo;s Hyde and its variant Nate Finch&amp;rsquo;s blog. It was heavily restructured with modularised page layouts for easier maintenance and modification. Hyde-hyde offers awesome features such as nice colour tone, code highlighting, Font-Awesome 5&amp;rsquo;s sidebar icons), a cool portfolio page, more choices for commenting (e.g. GraphComment, Disqus).</description>
    </item>
    
    <item>
      <title>LaraMod</title>
      <link>https://xiangxiaoxx.github.io/portfolio/laramod/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://xiangxiaoxx.github.io/portfolio/laramod/</guid>
      <description>LaraMod is another modularisation effort to systematically organising a Laravel based project. The idea stems from my struggle to structure a Laravel-based project so that I can work effectively on individual modules whilst keeping Laravel codebase intact as much as possible and also keeping the project&amp;rsquo;s codebase separate from Laravel.</description>
    </item>
    
  </channel>
</rss>