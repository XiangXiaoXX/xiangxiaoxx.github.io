<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on </title>
    <link>https://xiangxiaoxx.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 15 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://xiangxiaoxx.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>深度学习笔记</title>
      <link>https://xiangxiaoxx.github.io/posts/note/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://xiangxiaoxx.github.io/posts/note/</guid>
      <description>正则化 $g(w)=||w||1=\sum\limits{k}|w_i|$  $g(w)=||w||1^2=\sum\limits{k}|w_i^2|$
L1范数和L2范数 &amp;emsp;&amp;emsp;L1正则化会让参数变得更加稀疏，而L2正则化不会。 &amp;emsp;&amp;emsp;所谓参数变得稀疏是指会有更多参数变为0，这样可达到特征选取的功能。之所以L2正则化不会让参数变得稀疏的原因是，当参数很小时，这个参数的平方基本上就可以忽略了，于是模型不会进一步将这个参数调整为0。
偏置不需要正则化 &amp;emsp;&amp;emsp;同时，偏置是不应该被正则化的。正则化主要是为了防止过拟合，而过拟合一般表现为模型对于输出的微小改变产生巨大差异，而这主要是某些参数的w过大的原因，偏置对模型的曲率并没有任何贡献（模型的曲率是由w决定的，b对输入进行求导时，是直接约掉的），通过对||w||进行惩罚，才能缓解这种问题。 &amp;emsp;&amp;emsp;对||b||进行惩罚，其实是没有作用的。b对于所有对数据都是一视同仁的，而w会对不同的数据产生不一样的加权。 &amp;emsp;&amp;emsp;网上查了查相关的解释，还发现这是一道面试题。具体问法为，在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚。 &amp;emsp;&amp;emsp;《deep learning》书上的解释为：
&amp;gt;The biases typically require less data than the weights to ﬁt accurately. Each weight speciﬁes how two variables interact. Fitting the weight well requires observing both variables in a variety of conditions. Each bias controls only a single variable. This means that we do not induce too much variance by leaving the biases unregularized. Also, regularizing the bias parameter scan introduce a signiﬁcant amount of underﬁtting.</description>
    </item>
    
  </channel>
</rss>